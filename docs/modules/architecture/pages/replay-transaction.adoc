= Transaction Replay. [[replay_transaction]]

The Transaction-Replay helps to re-invoke the transaction that stopped due to xref:replay-transaction.adoc#resource-unavailable[resource-unavailable] issues.
It guarantees eventual consistency.

[[resource-unavailable]]
====
*Resource-Unavailable* term in the Microservice Architecture

In a microservice architecture, a *"resource-unavailable"* issue typically refers to a situation where one microservice is unable to access a necessary resource.
This resource could be another microservice, a database, a third-party API, or any other external system or service that the microservice depends on.
====

If a task is failed due to Resource Unavailable, it can be retried most of the time instead of stopping the task with en exception.
Because It is not easy like in a standalone application with a single database.
If one of atomic transactions is failed, other all atomic transactions that have been successfully executed should state back (undo).
Replaying tasks (transactions/operations) helps to have the eventual consistency.

For identifying the operation is a retryable operation or not, it is used xref:framework:non_retryable_executor_exception.adoc[NonRetryableExecutorException] and xref:framework:retryable_executor_exception.adoc[RetryableExecutorException].
If the executor throws and `RetryableExecutorException` the saga engine keep the transaction in the event-store for retrying.

NOTE: Other that the Stacksaga engine also might face for resource-unavailable issues most rarely due to using a database as the event-store.
It's a most important problem when you use a saga orchestration engine with event sourcing.
Stacksaga provides a reliable solution for that too.
That is discussed under the xref:dual_consistency_problem_of_sec_in_microservice.adoc[dual-consistency] problem topic.

== How to manage replaying?

Replaying transactions is a batch process that executes with a scheduler.
To replay the transactions, A schedule is used that you can configure.

In the event-store, there can have transactions that waiting to be replayed, and those transactions are exposed for replaying when the scheduler is triggered.
But as per architecture, who does trigger the scheduler within the running on instances?
Due to the replaying is batch process, it can not be done in parallel by the many instances.
That means all the available instances can't execute at the same time the transactions that are waiting to be replayed.
One service should execute the replaying process.
Instead of using one instance for doing all replaying related, StackSaga uses a hybrid way.

There are 2 types of implementations for retrying transactions in StackSaga.

* Master and Slave architecture.
* Static Master and Slave architecture.

====

IMPORTANT: Sometimes you might think that why can't run the transactions by the own instance of each individual transaction at the same time? +
The short answer is it can't be done due the instances are xref:replay-transaction.adoc#ephemeral[Ephemeral] in nature in the microservice architecture. +
For instance, just imagine there are 10 instances running on, and there are 100 transactions have been stopped temporarily by them due to some network issues.
After a while, the scheduler is triggered for replying the transactions on each instance.
At that moment it can have different instances count due to scaling up or scaling down based on the load.
Just imagine there was instance called "a" that made some retryable transactions before triggering the scheduler at it is might not be running when the scheduler is triggered.
Then the transactions that made by the instance called "a" cannot be run ever.
(Because the instance is identified with a random generated id and the transaction is stored with that instance id when the transaction is initiated).
Then some transactions can be missing.
This is one of the major reasons that replaying cannot involve all available instances at the same time.

====

[[ephemeral]]
====
*Ephemeral* behavior of the instances.

In the context of microservices, ephemeral refers to the principle that a microservice can be created, destroyed, and replenished on-demand on a target easily, quickly, and with no side effects.
====

== Master and Slave architecture for retrying

This method refers to electing a leader (Master instance) withing the available instances.

== Static Master and Slave architecture for retrying

////

The following reasons are caused to Transaction Replay.

. IF the transaction executor was failed with <<NonRetryableExecutorException,NonRetryableExecutorException>>. +
Any <<executor_architecture,executor>> can be re-invoked in StackSaga.
After executing your logic inside the executor, you can provide to the <<SEC,SEC>> what should be done as the next based on your conditions.
IF the executed transaction is failed due to a retry-able exception that executor can be re-invoked.
That helps to have the eventual consistency of the entire transaction.

. IF a <<dual_consistency_problem_of_sec_in_microservice,chunk-data>> file is restored after every-store problem.

IF your application is a large one.
There can be a lot of retryable transactions from each service in the event-store.
Therefore, executing the retryable transactions will be a heavy process due to the bulk.
To overcome this problem, StackSaga shares all the retryable transactions within the available instances in the zone.
The architecture is quite the same as <<execution_chunk_protection_mechanism_with_the_help_of_eureka_service_registry,chunk-data file relocating>>.
To share the transactions within the available instances, StackSaga follows the master and slave architecture.

*How is the master node appointed with the help of Eureka Registry?*

For selecting the master node, StackSaga uses eureka client metadata.
When the instance is started, StackSaga adds the timestamp as a metadata to the Eureka instance Info.
Then all the instances know who is the oldest instance in the zone.
The older instance will be appointed as the master node by itself.

image::stacksaga-unit-test-Transaction-Replay-Architecture-MI.drawio.svg[alt="StackSaga Transaction Replay Architecture",height=300]


* pass:[<span class="rounded-number">1</span>] Master gets the service registry from the eureka cache, and allocates retryable-transactions in the event-store for each available instance.
In the diagram, instance-1 makes the retryable-transactions allocation (you can configure the allocation count) for instance-2, instance-3, and instance-4.

* pass:[<span class="rounded-number">2</span>] After making the allocation for each.
the master notifies to each instance by making http requests.

* pass:[<span class="rounded-number">3</span>]  Then each instance starts the executing the allocated retryable-transactions bulk.

NOTE: Each availability zone has a master node.

After becoming as the master node, the instance has a special responsibility other than the slaves.
Here there is an allocation process by the master for other instances in the zone.

The slaves try to invoke the *allocated* retryable transactions for that particular instance by the master node.
////

