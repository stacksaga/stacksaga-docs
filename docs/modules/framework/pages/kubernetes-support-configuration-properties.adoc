:keywords: SatckSaga,Configuration Properties ,spring boot,spring cloud, saga design pattern,saga orchestration spring boot
:description: Spring boot StackSaga kubernetes-Support Configuration Properties

= StackSaga kubernetes-Support Configuration Properties

[cols="~,~,~,70h"]
|===
|Property Name|Default Value|Type|Description

|`stacksaga.cloud.k8s.serviceHost` | `${spring.application.name}` | String | The host name for communicate internally. If the service name is not same as the `${spring.application.name}` value, mentioning the serviceHost is required.
|`stacksaga.cloud.k8s.namespace` | `default` | String | The namespace that the application is deployed.
|`stacksaga.cloud.k8s.zoneTopologyName` | `topology.kubernetes.io/zone` |String | In StackSaga it is necessary to be aware of the location the instance is deployed in the cluster for some reason lime transaction indexing in the event store, retrying etc. therefore `stackSaga-kubernetes-support` access the kubernetes Api internally to fetch the topology related data. By default the zone data is added under the label `topology.kubernetes.io/zone` in kubernetes nodes. link:stacksaga_in_kubernetes.adoc#Leader-Election-based-configuration[read more...]
|`stacksaga.cloud.k8s.regionTopologyName` | `topology.kubernetes.io/region` |String | In StackSaga it is necessary to be aware of the location the instance is deployed in the cluster for some reason lime transaction indexing in the event store, retrying etc. therefore `stackSaga-kubernetes-support` access the kubernetes Api internally to fetch the topology related data. By default the region data is added under the https://kubernetes.io/docs/reference/labels-annotations-taints/:[Well-Known label] `topology.kubernetes.io/region` in kubernetes nodes. link:stacksaga_in_kubernetes.adoc#Leader-Election-based-configuration[read more...]
|`stacksaga.cloud.k8s.leader-election.leaseDuration` | `10 Minutes` | Duration | This parameter defines the duration for which a leader lease is held. A leader lease is essentially a lease that grants a node the authority to act as the leader for a specific period. During this lease duration, the leader node is responsible for handling requests and coordinating the actions of other nodes in the cluster. If the leader fails to renew its lease within the specified duration, the leadership is relinquished, and another node can take over as the leader. Setting an appropriate lease duration is crucial to balancing the trade-off between resilience and responsiveness in the system. https://kubernetes.io/docs/concepts/architecture/leases/:[read more about lease in k8s]
|`stacksaga.cloud.k8s.leader-election.renewDeadline` | `5 Minutes` | Duration | The renew deadline is the deadline by which the current leader must renew its lease to maintain its leadership status. If the leader fails to renew its lease before this deadline, it forfeits its leadership status, and a new leader may be elected. This parameter helps ensure that leadership transitions occur promptly in the event of leader failures or network partitions. https://kubernetes.io/docs/concepts/architecture/leases/:[read more about lease in k8s]
|`stacksaga.cloud.k8s.leader-election.retryPeriod` | `2 Minutes` | Duration | This parameter specifies the interval at which nodes attempt to acquire or renew leadership leases if they are not currently the leader. If a node fails to acquire or renew the lease, it will retry after the retry period has elapsed. The retry period helps prevent excessive contention for leadership and provides a mechanism for nodes to recover from transient failures or network issues. https://kubernetes.io/docs/concepts/architecture/leases/:[read more about lease in k8s]
|`stacksaga.cloud.k8s.leader-election.continuesRetryCount` | `5` | int | how much time should be retried when connection is failed?. if the connection is failed continuously for given times, the application will be terminated.

|`stacksaga.connect.adminUrls` |["http://localhost:4444/"] |List<String>|The urls for accessing Admin dashboard. If there are multiple instances of admin-server, all the urls can be provided as a list.
|`stacksaga.connect.adminUsername` | icon:circle[role=red,1x] non  | String | The username of the xref:admin:create_service_user.adoc[service-user] that has the authority to access the admin-server. See xref:admin:create_service_user.adoc[how to create service user in admin server.]
|`stacksaga.connect.adminPassword` | icon:circle[role=red,1x] non  | String | The password of the xref:admin:create_service_user.adoc[service-user] that has the authority to access the admin-server. See xref:admin:create_service_user.adoc[how to create service user in admin server.]
|`stacksaga.connect.maxAttempts` | 3  | int | How many times should attempt to make the request if the request is failed? If you have configured multiple admin urls, the attempts will be distributed evenly among them.
|`stacksaga.connect.backoffDelay` | 3_000L  | long (in ms) | The Constant delay (defined in milliseconds) before every retry attempt.
|`stacksaga.connect.backoffMaxDelay` | 3_000L  | long (in ms) | The maximum limit for the delay between retries (To prevent excessive delays).
|`stacksaga.connect.backoffMultiplier` | 2  | int |  The factor by which the delay increases in the exponential policy.
|`stacksaga.cloud.transactionRetryCron` | 0 0/5 * * * ?  | Cron String |  The scheduler for replaying the transactions that are waiting to be run in the event-store.(The transactions that have been temporally stopped due to network issues). This configuration is used only oof the instance is the leader in the region.
|`stacksaga.cloud.applicationStabilizationPeriod` | 5  | Duration by minutes | The scheduler that you configured for retrying the transaction  (`stacksaga.cloud.transactionRetryCron`) is worked after some delay. Because the instance should be stable for acting as the leader. This configuration is used only oof the instance is the leader in the region.
|`stacksaga.cloud.crashedTransactionRestoreRetentionHours` | 12  | Duration by hours | How long the transaction should be kept waiting to determine whether the transaction unexpectedly crashed. The value should be in hours. If there are some transactions in the event-store that have been shared for replaying but even after 12-hours (configured time,) that transaction has not been retried with that token. This is a very rare case. For instance, after receiving the transaction for replaying by the one of available instances, the instance goes down due to a power cut without executing the transaction. But the leader has been updated as the transaction has been shared to an instance for doing replay. Due to that, the leader doesn't invoke those transactions again until the transaction is updated by the received instance or the `crashedTransactionRestoreRetentionHours` is exceeded. Before collecting the transactions that should be retried, the leader checks that if there are some transactions that exceed the `crashedTransactionRestoreRetentionHours` time and those transactions update again as to be eligible for retrying.
|`stacksaga.cloud.transactionRetryRetentionPeriod` | 60 * 10  | Duration by | How many times should a transaction be retried within a certain period of time?
|`stacksaga.cloud.transactionRetryBatchSize` | 5000  | long | When the leader fetches the transactions from the event-store that should be replayed, how much should be the batch size. For instance, If there are 100_000 transactions on the event-store to be replayed, those 100_000 transactions loaded in to the Transaction-Retry-Queue 20 times.
|`stacksaga.cloud.transactionRetryOrder` | ASC  | Enum | Which order the transactions should be replayed based on the transaction initiate time. By default, ascending order is usd. (First-n > First-out)
|`stacksaga.cloud.transactionRetryQueueCapacity` | 10_000  | int | When the transactions are collected from the event-store that should be retried, a loop is triggered until the transactions count zero that should be retried. The transactions are fetched as batches for retrying, and those batches are added to the transaction queue for share those transactions to the available instances. The maximum size of that pool is configured here.


|===

[[crashedTransactionRestoreRetentionHours]]
== Crashed Transaction Restore Retention Hours

Duration by hours | How long the transaction should be kept waiting to determine whether the transaction unexpectedly crashed.
The value should be in hours.
If there are some transactions in the event-store that have been shared for replaying but even after 12-hours (configured time,) that transaction has not been retried with that token.
This is a very rare case.
For instance, after receiving the transaction for replaying by the one of available instances, the instance goes down due to a power cut without executing the transaction.
But the leader has been updated as the transaction has been shared to an instance for doing replay.
Due to that, the leader doesn't invoke those transactions again until the transaction is updated by the received instance or the `crashedTransactionRestoreRetentionHours` is exceeded.
Before collecting the transactions that should be retried, the leader checks that if there are some transactions that exceed the `crashedTransactionRestoreRetentionHours` time and those transactions update again as to be eligible for retrying.