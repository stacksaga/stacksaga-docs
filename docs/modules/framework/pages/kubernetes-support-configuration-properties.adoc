:keywords: SatckSaga,Configuration Properties ,spring boot,spring cloud, saga design pattern,saga orchestration spring boot
:description: Spring boot StackSaga kubernetes-Support Configuration Properties

= StackSaga kubernetes-Support Configuration Properties

[cols="~,~,~,70h"]
|===
|Property Name|Default Value|Type|Description

|`stacksaga.cloud.k8s.serviceHost` | `${spring.application.name}` | String | The host name for communicate internally. if the service name is not same as the `${spring.application.name}` value, mentioning the serviceHost is required.
|`stacksaga.cloud.k8s.namespace` | `default` | String | The namespace that the application is deployed.
|`stacksaga.cloud.k8s.zoneTopologyName` | `topology.kubernetes.io/zone` |String | In StackSaga it is necessary to be aware of the location the instance is deployed in the cluster for some reason lime transaction indexing in the event store, retrying etc. therefore `stackSaga-kubernetes-support` access the kubernetes Api internally to fetch the topology related data. by default the zone data is added under the label `topology.kubernetes.io/zone` in kubernetes nodes. link:stacksaga_in_kubernetes.adoc#Leader-Election-based-configuration[read more...]
|`stacksaga.cloud.k8s.regionTopologyName` | `topology.kubernetes.io/region` |String | In StackSaga it is necessary to be aware of the location the instance is deployed in the cluster for some reason lime transaction indexing in the event store, retrying etc. therefore `stackSaga-kubernetes-support` access the kubernetes Api internally to fetch the topology related data. by default the region data is added under the https://kubernetes.io/docs/reference/labels-annotations-taints/:[Well-Known label] `topology.kubernetes.io/region` in kubernetes nodes. link:stacksaga_in_kubernetes.adoc#Leader-Election-based-configuration[read more...]
|`stacksaga.cloud.k8s.regionTopologyName` | `topology.kubernetes.io/region` |String | In StackSaga it is necessary to be aware of the location the instance is deployed in the cluster for some reason lime transaction indexing in the event store, retrying etc. therefore `stackSaga-kubernetes-support` access the kubernetes Api internally to fetch the topology related data. by default the region data is added under the https://kubernetes.io/docs/reference/labels-annotations-taints/:[Well-Known label] `topology.kubernetes.io/region` in kubernetes nodes. link:stacksaga_in_kubernetes.adoc#Leader-Election-based-configuration[read more...]
|`stacksaga.cloud.k8s.leader-election.leaseDuration` | `10 Minutes` | Duration | This parameter defines the duration for which a leader lease is held. A leader lease is essentially a lease that grants a node the authority to act as the leader for a specific period. During this lease duration, the leader node is responsible for handling requests and coordinating the actions of other nodes in the cluster. If the leader fails to renew its lease within the specified duration, the leadership is relinquished, and another node can take over as the leader. Setting an appropriate lease duration is crucial to balancing the trade-off between resilience and responsiveness in the system. https://kubernetes.io/docs/concepts/architecture/leases/:[read more about lease in k8s]
|`stacksaga.cloud.k8s.leader-election.renewDeadline` | `5 Minutes` | Duration | The renew deadline is the deadline by which the current leader must renew its lease to maintain its leadership status. If the leader fails to renew its lease before this deadline, it forfeits its leadership status, and a new leader may be elected. This parameter helps ensure that leadership transitions occur promptly in the event of leader failures or network partitions. https://kubernetes.io/docs/concepts/architecture/leases/:[read more about lease in k8s]
|`stacksaga.cloud.k8s.leader-election.retryPeriod` | `2 Minutes` | Duration | This parameter specifies the interval at which nodes attempt to acquire or renew leadership leases if they are not currently the leader. If a node fails to acquire or renew the lease, it will retry after the retry period has elapsed. The retry period helps prevent excessive contention for leadership and provides a mechanism for nodes to recover from transient failures or network issues. https://kubernetes.io/docs/concepts/architecture/leases/:[read more about lease in k8s]
|`stacksaga.cloud.k8s.leader-election.continuesRetryCount` | `5` | int | how much time should be retried when connection is failed?. if the connection is failed continuously for given times, the application will be terminated.

|`stacksaga.connect.adminUrls` |["http://localhost:4444/"] |List<String>|The urls for accessing Admin dashboard. If there are multiple instances of admin-server, all the urls can be provided as a list.
|`stacksaga.connect.adminUsername` | icon:circle[role=red,1x] non  | String | The username of the xref:admin:create_service_user.adoc[service-user] that has the authority to access the admin-server. See xref:admin:create_service_user.adoc[how to create service user in admin server.]
|`stacksaga.connect.adminPassword` | icon:circle[role=red,1x] non  | String | The password of the xref:admin:create_service_user.adoc[service-user] that has the authority to access the admin-server. See xref:admin:create_service_user.adoc[how to create service user in admin server.]
|`stacksaga.connect.maxAttempts` | 3  | int | How many times should attempt to make the request if the request is failed?
If you have configured multiple admin urls, the attempts will be distributed evenly among them.
|`stacksaga.connect.backoffDelay` | 3_000L  | long (in ms) | The Constant delay (defined in milliseconds) before every retry attempt.
|`stacksaga.connect.backoffMaxDelay` | 3_000L  | long (in ms) | The maximum limit for the delay between retries (To prevent excessive delays).
|`stacksaga.connect.backoffMultiplier` | 2  | int |  The factor by which the delay increases in the exponential policy.

|===
