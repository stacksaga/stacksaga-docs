= stacksaga-cassandra-support

`stacksaga-cassandra-support` is one of Stacksaga db support(event store support) implementations as per the architectural diagram.

image::agent/stacksaga-diagram-stacksaga-components-database-support.svg[]

It is used for the orchestrator service along with the starter dependency if the orchestrator service uses the Cassandra database as their primary database.
This library provides all the facilities for accessing the Cassandra database for the Stacksaga engine.

Here is the way that you can add the library into your existing orchestrator application as a dependency.

.Adding `stacksaga-cassandra-support` as a dependency
[source,xml]
----
<dependency>
    <groupId>org.stacksaga</groupId>
    <artifactId>stacksaga-cassandra-support</artifactId>
    <version>${org.stacksaga.version}</version>
</dependency>
----

== How the transaction data is sorted

As mentioned above, the primary responsibility of the Cassandra database support implementation is providing the facilities for saving the transaction data on the event store.
Saving the transaction data approach is quite different from the SQL support implementations due to the database's nature like data modeling, Denormalized Schema, clustering, partitioning, etc.

// The main purpose of using the Cassandra database is to handle high throughput.

All the transactions are saved in 4 tables in the Cassandra database.

. `es_transaction` table
. `es_transaction_tryout` table.
. `transaction_recovery_retention` tables.
.. `es_{region_name}_transaction_recovery_retention_1` table.
.. `es_{region_name}_transaction_recovery_retention_2` table.
.. `es_{region_name}_transaction_recovery_retention_3` table.
. `es_transaction_retry_retention` tables.
.. `es_{region_name}_transaction_retry_retention_1` table.
.. `es_{region_name}_transaction_retry_retention_2` table.
.. `es_{region_name}_transaction_retry_retention_3` table.


=== es_transaction table

`es_transaction` is the main table that transaction's meta-data is saved in event store based on the *transaction-Id* as the *Partition-Key*.
All the transactions are saved in this table as *https://cassandra.apache.org/doc/stable/cassandra/data_modeling/data_modeling_refining.html[Single-Row Partition]*.
Single-Row Partition approach refers to handling millions of transactions without making Database-Hotspot in your Cassandra database's nodes.
Because the transactions are shared withing the all available nodes efficiently like below.

image:framework:agent/cassandra/stacksaga-diagram-cassandra-managing-throughput.drawio.svg[alt="StackSaga cassandra managing throughput"]

IMPORTANT: Even the Single-Row Partition helps to overcome the database hotspot problem, The data cannot be fetched from the database without knowing the exact transaction key.
It makes a trouble for fetching the data that should be retried from all the transactions from the entire table.
It is discussed in the xref:framework:agent/cassandra-agent.adoc[`cassandra-stacksaga-agent`] section.

=== es_transaction_tryout table

*es_transaction_tryout* is the table that transaction's tryout-data is saved in based on the *transaction-Id* as the *Partition-Key*.
transaction tryouts are saved under the *transaction-Id* with *Multi-Row Partition* approach.
due to the tryout data is saved based on the *transaction-Id*, tryout data is also saved in the same node that the transaction has been saved.
it helps to optimize the network latency, and all the data(`es_transaction` and `es_transaction_tryout`) goes to the same node.

image:framework:agent/cassandra/stacksaga-diagram-cassandra-es-transaction-tryout-table.svg[alt="StackSaga cassandra managing throughput"]

As you can see in the above diagram, the tryout data is saved in the same node that the transaction has been saved.
If the `tx-8dKz7LpJ2Q` transaction metadata is saved in the `Node-1` node, the `tx-8dKz7LpJ2Q` transaction tryout data is also saved in the `Node-1` node.

== es_transaction_retention table.

== transaction_recovery_retention tables.

// As mentioned above, the transaction cannot be identified based on the status like in relational database databases due to the structure of the transaction data is saved.
// Because Cassandra distributes data across multiple nodes based on the partition key.
// So after adding the transactions into the transaction table, it cannot be filtered as a bulk.
// Therefore, there should have a mechanism to identify the missing transactions in another way.

Unlike relational databases, transactions in Cassandra cannot be identified based on a field like status due to the way data is distributed across multiple nodes using a partition key.
This distribution prevents bulk filtering of transactions directly from the transaction table.

===  Why Bulk Filtering is Not Possible?

Partition-Based Distribution:: Cassandra shards data across multiple nodes, making global queries on large datasets inefficient.
No Centralized Indexing for Status-Based Queries:: Since transactions are stored based on partition keys, retrieving missing transactions requires querying multiple partitions, which is not feasible for large-scale systems.

=== Alternative Mechanism to Identify Missing Transactions.

Since transactions cannot be filtered in bulk due to Cassandraâ€™s partition-based distribution, an alternative mechanism is required to track missing transactions effectively.

. Use separate recovery tables
* Instead of relying on status-based filtering, transactions that require tracking should be stored in separate recovery tables.
* These tables can act as a centralized reference for identifying transactions that may not have completed.

. Time-Based Retention Mechanism

* Each transaction is retained for a specific time window after being added to the transaction table.
* If a transaction fails to complete within this configured time frame, it is considered missing, and it will be exposed for re-invoking.
. Agent-Based Recovery Mechanism

* Deploy an agent service that collects pending transactions from the `transaction_recovery_retention` table.
* The agent then forwards the missing transactions to the orchestrator service for re-invoke.


////
Storing the transaction data temporarily in the `transaction_recovery_retention` tables is the way to identify the missing transactions.

When the transaction is initialized, the transaction data is saved in one of the `transaction_recovery_retention` tables temporarily.
And then the transaction data remains until the transaction is successfully completed(primary-execution is successful or compensating is successful).
When the transaction is successfully completed, the transaction data is removed from the `transaction_recovery_retention` table.

How can the transactions be missed?

just imagine that while the transaction is being processed, the running node goes down instantly due to a power outage or hardware failure.
Then the transactions that were running or the transactions were in the queue for running were vanished, and it cannot be identified anymore if we had not saved the running transactions in the `transaction_recovery_retention` table.

How the missing transactions are identified with `transaction_recovery_retention` tables?
////

As described earlier, when a transaction is initialized, its data is stored in one of the `transaction_recovery_retention` tables.
If the transaction completes successfully, its data is removed from the table upon process completion.
However, if a transaction does not reach completion, its data remains in the `transaction_recovery_retention` table, making it a potential candidate for recovery.

Scenario: Identifying Missing Transactions::
Consider a scenario where 1,000 transactions are executed within a specified time period across multiple nodes.
Ideally, all transactions that complete their journey are removed from the `transaction_recovery_retention` table.
However, if even one transaction fails to complete, it will persist in the table.
Then the transaction is exposed to the agent application after the configured time period, and then the agent application will collect the missing transactions from the respective `transaction_recovery_retention` table and shares them withing the available orchestrator service to re-invoke.
This could happen due to various reasons such as system failures, network issues, or unexpected interruptions.

=== `transaction_recovery_retention` table selection formula.

As mentioned above, there are 3 `transaction_recovery_retention` tables for adding the transactions temporarily.
In cassandra implementation, the *Transaction recovery retention time* is not a fixed one like in other database implementations.
The *Transaction recovery retention time* is oscillated between a range.

image::agent/cassandra/stacksaga-diagram-stacksaga-cassandra-how-transactions-saved-for-recovery.svg[]

Just imagine if you configure the *Transaction recovery retention time* to be 8 hours.
The *Transaction recovery retention time* will be withing the range of 4 hours to 12 hours.

*How does that happen?*

If the *Transaction recovery retention time* has been mentioned as 8 hours(480 minutes), 3 scheduled can be triggered withing a day for collecting and re-invoke the missing transactions like the diagram shows.

. 1st schedule at: 00:00
. 2nd schedule at: 08:00
. 3rd schedule at: 16:00

To determine whether a transaction has sufficient time to complete its journey, the system uses the middle time of the configured duration as the boundary point.
This boundary helps classify transactions into different recovery schedules.

*Transaction Placement Logic*

. Transactions Behind the Boundary (Back of the Boundary)

* These transactions have already passed the boundary point.
* They are scheduled for the next upcoming recovery schedule.
. Transactions Ahead of the Boundary
* These transactions were initialized after the boundary point.
* They are scheduled for the recovery schedule after the next upcoming schedule to allow more time for completion.

The middle time is considered as the boundary point for determining that the transaction has sufficient time to complete their Journey.
The transactions that are at the back of the boundary go to the next upcoming schedule.
And the transactions that are ahead of the boundary go to the schedule after the next upcoming schedule.

[cols="^1,^1,^1,^1,^1",options="header"]
|===
| Transaction | Initialization Time | Time Until End (16:00)  | Position Relative to Boundary | Recovery Schedule
| T4         | 11:30               | 4 hours 30 minutes                | Back of the boundary         | Next upcoming schedule
| T5         | 12:30               | 3 hours 30 minutes                 | Ahead of the boundary        | Schedule after the next
| T6         | 15:59               | 1 minute                   | Ahead of the boundary        | Schedule after the next
|===





// IMPORTANT: *Handling False Positives in Transaction Recovery & the Role of Idempotency* +
// Even if the transactions that are in the `transaction_recovery_retention` table are considered as missing, Sometimes it may not be so.
// For instance, just imagine that transition is still in the queue waiting for execution due to the respective orchestrator service is too busy.
// In such a case, the system assumes that the transaction was missing and the transaction is invoked.
// Then the transaction(entirely or some atomic executions) can be executed multiple times.
// This is where the idempotency comes in.
// If you have followed the idempotency in your atomic executions of the transaction, it will not be a problem.



=== Handling False Positives in Transaction Recovery & the Role of Idempotency

While transactions that remain in the `transaction_recovery_retention` table are generally considered missing, this is not always the case.

Scenario: Transactions Delayed but Not Missing::
Consider a situation where a transaction is still in the queue, waiting for execution because the **respective orchestrator service is too busy**.
In this case:

1. The system mistakenly **assumes the transaction is missing** since it has not been removed from the `transaction_recovery_retention` table within the expected time frame.
2. As a result, the system **triggers a recovery process**, re-invoking the transaction.
3. This can lead to the transaction(or certain *atomic executions*) **executed multiple times**, causing unintended duplicate operations.

IMPORTANT: This is one of possible ways the transactions can be executed multiple times.
To prevent these kinds of unintended duplicate executions, **xref:architecture:idempotency.adoc[idempotency]** should be implemented at the atomic execution level of the transaction.

////

== es_transaction_retry_bucket table

the *es_transaction_retry_bucket* tables are created based on the given configuration for `stacksaga.cloud.agent.retry-fixed-delay`.
the default fixed-delay value is 2 minutes.
that means the retry scheduler is triggered every 2 minutes, so withing one hour the retry scheduler is triggered 30 times.
then 30 tables are created when the application is started if the tables are not existed like, +

* *es_transaction_retry_bucket_0*
* *es_transaction_retry_bucket_2*
* *es_transaction_retry_bucket_4*
* *es_transaction_retry_bucket_6*
* *es_transaction_retry_bucket_8*
* *es_transaction_retry_bucket_10*
* *es_transaction_retry_bucket_12*
* *es_transaction_retry_bucket_14*
* *es_transaction_retry_bucket_16*
* ....
* ....
* *1_54*
* *es_transaction_retry_bucket_56*
* *es_transaction_retry_bucket_58*

For instance, if you customize the `stacksaga.cloud.agent.retry-fixed-delay` value as 10, the table count will be 6 (60/10).

Let's see how the table is selected when the transaction is saved in one of es_transaction_retry_bucket tables.

When a transaction is initiated by the stacksaga-framework, the transaction is saved on es_transaction and es_transaction_tryout tables.
after that, the transaction should be saved in one of the es_transaction_retry_bucket tables.
just imagine if the transaction is initiated at `2022-01-01 00:00:00.000` the transaction is saved on the farthest es_transaction_retry_bucket table from that time.
according to this transaction, the table will be *es_transaction_retry_bucket_60*.

IMPORTANT: The reason for selecting the farthest table is that still the framework has not identified the transaction has a *retryable-error* even the transaction is saved a table that can be exposed for retrying.
and the reason for adding every transaction to one of the es_transaction_retry_bucket tables is that the transaction cannot be caught based on the STATUS of the transaction due to StackSaga doesn't save the transaction based on the Transaction status.
Saving the transaction based on the status can be increased the network latency, StackSaga is responsible for saving the metadata in maximum performance to reduce the overhead of using a third-party framework for managing a transaction.
and also, Saving the transaction based on the status can be caused to have a hotspot issue if the system is a large one. +
For instance, if one million concurrent transactions come to the system and those transactions are failed due to a utility service's failure, the framework has to add a metadata of each transaction to a table.
the problem is that due to the time exact same (The token that Cassandra generates will be the same) for all transactions that one million transactions goes to the same node.
then it can lead to a hotspot issue.

if the transaction is processed successfully without any retryable error, the record will be deleted from the table at the end of the transaction.
but if there is an

es_transaction_retry_bucket_* table is used for identifying the retryable transactions.
This table is used in StackSaga in a quite different approach from the regular approach that a table used.
This table is used as a data bucket. that means the data that is stored in this table is deleted after using.

es_transaction_retry_bucket is a not a single table. it's actually the prefix of the table name.

you know that already prefixed tables are used for identifying the retryable transactions.
so when a transaction is initiated, it is saved in the es_transaction_retry_bucket table apart from the es_transaction and es_transaction_tryout table.

////

