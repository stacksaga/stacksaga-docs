= Kubernetes

== Overview

When your application is deployed in Kubernetes with StackSaga the core architecture is changed a bit.
in the StackSaga architecture there is a spacial part called xref:architecture:replay-transaction.adoc[transaction replay (retry)].
you know already that to get the responsibility and perform retrying, there should be one leader for the given cluster of servers. in simple StackSaga should select one of the services (within the same service name) for perform the retrying.
it is called leader election. the leader election part was quite simple when you are in the spring pure microservice environment.
It was used the Eureka service registry for electing the leader. but if we move to the Kubernetes environment, the best practices and the recommitted way is using built-in https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/[ service discovery capabilities.]

NOTE: If you are prefer to go with default pure spring cloud architecture with Eureka it also can be done without any doubt as usual.

If you are going with kubernetes' built-in service discovery capabilities Eureka-Server is not used anymore (https://docs.spring.io/spring-cloud-kubernetes/reference/discovery-client.html[read Spring DiscoveryClient for Kubernetes]) and the leader election part is done by using the https://kubernetes.io/docs/concepts/architecture/leases/[Kubernetes Leases Leader election]. to overcome that inside of the StackSaga, the framework uses https://github.com/kubernetes-client/java[kubernetes-client] internally.

== Migrating to StackSaga kubernetes discover

In general architecture `stack-saga-discovery-client` dependency has been used for discover related things. when you are moving with kubernetes discover, `stack-saga-k8s-discovery` dependency is replaced.

[source,xml]
----
<dependency>
    <groupId>org.stacksaga</groupId>
    <artifactId>stack-saga-k8s-discovery</artifactId>
    <version>1.0.0</version>
</dependency>
----

IMPORTANT: In general architecture a kubernetes service is created to make the Internal communication within the services.
(Service to Service communication and Api-Gateway to other downstream services).
Due to the eureka is not used in kubernetes by default the framework uses the service name as the host to communicate to other instances.
When you create the https://kubernetes.io/docs/concepts/services-networking/service/:[kubernetes service], the service name that is used for the service (`metadata.name`) is different to application name (`spring.application.name`) make sure to mention it by using xref:kubernetes-support-configuration-properties.adoc[`stacksaga.cloud.k8s.serviceHost`] property.

This dependency helps to leader election part and retrying by connect with kubernetes' API. due to accessing the kubernetes' API (https://kubernetes.io/docs/reference/access-authn-authz/rbac/[RBAC Authorization]), you have to bind a https://kubernetes.io/docs/concepts/security/service-accounts/[service account] with https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-and-clusterrole[a ClusterRole].


[#Leader-Election-based-configuration]
== Leader Election based configuration

**ServiceAccount**

[source,yaml]
----

apiVersion: v1
kind: ServiceAccount
metadata:
  name: order-service-service-account #the name of the service account.
  namespace: default #the namespace the application is deployed.
----

*ClusterRole*

[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: leader-election-lease-role #the name of the role.
rules:
  - apiGroups: [ "","coordination.k8s.io" ] # <1>
    resources: [ "endpoints","leases" ]
    verbs: [ "get", "list", "watch", "create", "update", "patch", "delete" ]
----

IMPORTANT: Due to the fact that the Kubernetes java client uses leases for leader election, `coordination.k8s.io` api-group should be added into the `apiGroups` array. because the lease object related data is provided through the `"coordination.k8s.io"` api. https://kubernetes.io/docs/concepts/architecture/leases/[read more...] +
See in your console. *`#$ kubectl api-resources | grep leases#`*



*ClusterRoleBinding*

[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: order-service-service-account-leader-election-lease-role
  namespace: default
#bind the service account with role
subjects:
  - kind: ServiceAccount
    name: order-service-service-account
    namespace: default
roleRef:
  kind: Role
  name: leader-election-lease-role
  apiGroup: rbac.authorization.k8s.io
----

After creating the user account related necessary manifests, make sure to bind the user account with the Deployment. (https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/[configure-service-account in k8s])

// todo: put and example for adding the service account fro the Deployment.

IMPORTANT: StackSga leader election is elected based on the region and also the service-name. and also kubernetes leader election supports only for withing the cluster. therefore one region only can have one kubernetes cluster.
for instance just imagine that you run 2 clusters and in both clusters your run same service with the same database. then both clusters will have tow different leaders for the same service at the same time. that means you brake the leader rule. the problem with it is that the leader fetches the retryable data for retrying (The retrying process is done batch vise) from the database related to the region and the service name. then at the same time the same transaction can be caught both leaders. then the transactions can be crashed due to the parallel execution.

NOTE: If you want to use https://docs.spring.io/spring-cloud-kubernetes/reference/leader-election.html[spring leader election dependency], you are totally free to use that as usual. because Stacksaga does not use spring's leader election object to leader election. and also any spring leader election dependency is not used internally in StackSaga. therefore you can implement your own leader election requirement.


====
TIP:  After adding `stack-saga-k8s-discovery` you can use it to access the *pod metadata*  and as well as the *node metadata* that kubernetes provides.

[source,java]
----

import io.kubernetes.client.openapi.models.V1Node;
import io.kubernetes.client.openapi.models.V1Pod;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Component;
import org.stacksaga.common.communication.SagaInstanceInfo;
import org.stacksaga.common.communication.ServiceInstanceInfo;
import org.stacksaga.kubernetes.KubernetesSagaInstanceInfo;

@Component
@RequiredArgsConstructor
public class InstanceDetailService {

    private final ServiceInstanceInfo serviceInstanceInfo; //<1>

    public void showInstanceInfo() {
        final SagaInstanceInfo sagaInstanceInfo = serviceInstanceInfo.getSagaInstanceInfo();
        if (sagaInstanceInfo instanceof KubernetesSagaInstanceInfo) { //<2>
            final KubernetesSagaInstanceInfo kubernetesSagaInstanceInfo = (KubernetesSagaInstanceInfo) sagaInstanceInfo; //<3>
            V1Pod v1Pod = kubernetesSagaInstanceInfo.getV1Pod(); //<4>
            System.out.println("v1Pod = " + v1Pod);
            V1Node v1Node = kubernetesSagaInstanceInfo.getV1Node(); //<5>
            System.out.println("v1Node = " + v1Node);
        }
    }
}
----

<1> Autowire the `ServiceInstanceInfo` bean.
<2> Call the `getSagaInstanceInfo()` method to get the assistance details and check that object is an instance of `KubernetesSagaInstanceInfo`. due to the adding the `stack-saga-k8s-discovery` dependency you will have a `KubernetesSagaInstanceInfo`.
<3> cast the object in to `KubernetesSagaInstanceInfo` object.
<4> Access the https://github.com/kubernetes-client/java/blob/master/kubernetes/docs/V1Pod.md[V1Pod] object.
<5> Access the https://github.com/kubernetes-client/java/blob/master/kubernetes/docs/V1Node.md[V1Node] object.
====